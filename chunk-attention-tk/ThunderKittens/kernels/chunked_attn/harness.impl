/**
 * Test harness for attn_chunk_first_tk
 *
 * Reads test data from file, runs kernel, compares against reference.
 *
 * Also supports CUDA-event timed benchmarking (kernel-only timing):
 *   - ./attn_chunk_first --bench --n-heads <H> --n-chunks <C> [--warmup N --iters N]
 *   - ./attn_chunk_first --sweep [--warmup N --iters N]
 */

#include <iostream>
#include <fstream>
#include <vector>
#include <cmath>
#include <cstring>
#include <sstream>
#include <iomanip>
#include <cuda_runtime.h>

// ============================================================================
// Configuration (must match kernel)
// ============================================================================
constexpr int MAX_N_SEQS = 64;  // 64 rows for full warpgroup utilization (64/4 = 16 rows per warp)
constexpr int CHUNK_SIZE = 64;
constexpr int D_HEAD = 128;
constexpr int BLOCK_THREADS = 128;  // 4 warps Ã— 32 = 128 threads = 1 warpgroup

// ============================================================================
// Utilities
// ============================================================================
#define CUDA_CHECK() do { \
    cudaError_t e = cudaGetLastError(); \
    if (e != cudaSuccess) { \
        fprintf(stderr, "CUDA error %s:%d: %s\n", __FILE__, __LINE__, cudaGetErrorString(e)); \
        exit(1); \
    } \
    cudaDeviceSynchronize(); \
} while(0)

void to_bf16(const float* src, bf16* dst, int n) {
    for (int i = 0; i < n; i++) dst[i] = __float2bfloat16(src[i]);
}

// Transpose Q from [n_seqs, n_heads, d_head] to [n_heads, n_seqs, d_head] for TMA
void transpose_q_for_tma(const float* src, bf16* dst, int n_seqs, int n_heads, int d_head) {
    for (int h = 0; h < n_heads; h++) {
        for (int s = 0; s < n_seqs; s++) {
            for (int d = 0; d < d_head; d++) {
                // src index: [s, h, d] = s * n_heads * d_head + h * d_head + d
                // dst index: [h, s, d] = h * n_seqs * d_head + s * d_head + d
                int src_idx = s * n_heads * d_head + h * d_head + d;
                int dst_idx = h * n_seqs * d_head + s * d_head + d;
                dst[dst_idx] = __float2bfloat16(src[src_idx]);
            }
        }
    }
}

static int parse_int_flag(int argc, char** argv, const char* flag, int default_value) {
    for (int i = 1; i + 1 < argc; i++) {
        if (strcmp(argv[i], flag) == 0) return std::atoi(argv[i + 1]);
    }
    return default_value;
}

static bool has_flag(int argc, char** argv, const char* flag) {
    for (int i = 1; i < argc; i++) if (strcmp(argv[i], flag) == 0) return true;
    return false;
}

static void set_device_from_flags(int argc, char** argv) {
    int dev = parse_int_flag(argc, argv, "--device", -1);
    if (dev >= 0) {
        cudaSetDevice(dev);
        CUDA_CHECK();
    }
}

static double flops_total(int n_seqs, int n_heads, int n_chunks) {
    // per (head, chunk): QK^T (2 flops/FMA) + (exp_scores)V (2 flops/FMA)
    // = 4 * n_seqs * chunk_size * d_head
    const double blocks = double(n_heads) * double(n_chunks);
    return blocks * 4.0 * double(n_seqs) * double(CHUNK_SIZE) * double(D_HEAD);
}

static double kv_bytes_total(int n_heads, int n_chunks) {
    // reads K and V once per (head, chunk): 2 tensors * (chunk_size*d_head) bf16 elements.
    // bf16 = 2 bytes -> bytes/block = 2 * chunk_size*d_head * 2 = 4*chunk_size*d_head
    const double blocks = double(n_heads) * double(n_chunks);
    return blocks * 4.0 * double(CHUNK_SIZE) * double(D_HEAD);
}

static float bench_kernel_ms(int n_heads, int n_chunks, int warmup, int iters) {
    const int n_seqs = MAX_N_SEQS;

    // Sizes
    const int q_size = n_seqs * n_heads * D_HEAD;
    const int kv_size = n_heads * CHUNK_SIZE * D_HEAD;
    const int out_size = n_heads * n_seqs * D_HEAD;
    const int ms_size = n_heads * n_seqs;

    // Allocate device memory
    bf16 *d_q;
    std::vector<bf16*> d_k(n_chunks), d_v(n_chunks);
    float *d_attns, *d_maxs, *d_sums;
    void **d_keys, **d_vals;
    int *d_offsets, *d_begins, *d_ends;

    cudaMalloc(&d_q, q_size * sizeof(bf16));
    for (int c = 0; c < n_chunks; c++) {
        cudaMalloc(&d_k[c], kv_size * sizeof(bf16));
        cudaMalloc(&d_v[c], kv_size * sizeof(bf16));
    }

    int total_out = n_chunks * out_size;
    int total_ms = n_chunks * ms_size;
    cudaMalloc(&d_attns, total_out * sizeof(float));
    cudaMalloc(&d_maxs, total_ms * sizeof(float));
    cudaMalloc(&d_sums, total_ms * sizeof(float));
    cudaMemset(d_attns, 0, total_out * sizeof(float));
    cudaMemset(d_maxs, 0, total_ms * sizeof(float));
    cudaMemset(d_sums, 0, total_ms * sizeof(float));

    // Simple deterministic init (all zeros) is fine for benchmarking.
    cudaMemset(d_q, 0, q_size * sizeof(bf16));
    for (int c = 0; c < n_chunks; c++) {
        cudaMemset(d_k[c], 0, kv_size * sizeof(bf16));
        cudaMemset(d_v[c], 0, kv_size * sizeof(bf16));
    }

    // Setup pointer arrays
    std::vector<void*> h_keys(n_chunks), h_vals(n_chunks);
    for (int c = 0; c < n_chunks; c++) { h_keys[c] = d_k[c]; h_vals[c] = d_v[c]; }
    cudaMalloc(&d_keys, n_chunks * sizeof(void*));
    cudaMalloc(&d_vals, n_chunks * sizeof(void*));
    cudaMemcpy(d_keys, h_keys.data(), n_chunks * sizeof(void*), cudaMemcpyHostToDevice);
    cudaMemcpy(d_vals, h_vals.data(), n_chunks * sizeof(void*), cudaMemcpyHostToDevice);

    // Metadata (each chunk covers all seqs)
    std::vector<int> h_begins(n_chunks, 0);
    std::vector<int> h_ends(n_chunks, n_seqs);
    std::vector<int> h_offsets(n_chunks);
    for (int c = 0; c < n_chunks; c++) h_offsets[c] = c * n_seqs;

    cudaMalloc(&d_begins, n_chunks * sizeof(int));
    cudaMalloc(&d_ends, n_chunks * sizeof(int));
    cudaMalloc(&d_offsets, n_chunks * sizeof(int));
    cudaMemcpy(d_begins, h_begins.data(), n_chunks * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_ends, h_ends.data(), n_chunks * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_offsets, h_offsets.data(), n_chunks * sizeof(int), cudaMemcpyHostToDevice);
    CUDA_CHECK();

    // Setup kernel globals
    using Globals = attn_chunk_first_globals<MAX_N_SEQS, D_HEAD>;
    Globals g {
        .attns = gl<float, -1, -1, -1, -1>(d_attns, 1, 1, total_out / D_HEAD, D_HEAD),
        .maxs = gl<float, -1, -1, -1, -1>(d_maxs, 1, 1, 1, total_ms),
        .sums = gl<float, -1, -1, -1, -1>(d_sums, 1, 1, 1, total_ms),
        .offsets = gl<int, -1, -1, -1, -1>(d_offsets, 1, 1, 1, n_chunks),
        .begins = gl<int, -1, -1, -1, -1>(d_begins, 1, 1, 1, n_chunks),
        .ends = gl<int, -1, -1, -1, -1>(d_ends, 1, 1, 1, n_chunks),
        .Q = make_gl<Globals::Q_gl>((uint64_t)d_q, 1, n_heads, n_seqs, D_HEAD),
        .keys = d_keys,
        .values = d_vals,
        .scale = 1.0f / sqrtf(float(D_HEAD)),
        .n_heads = n_heads
    };

    // Dynamic shared memory size (matches attn_chunk_first.cu allocations)
    // NOTE: K and V share the same buffer (reused sequentially)
    constexpr int NUM_WARPS = 4;
    constexpr int ROWS_PER_WARP = MAX_N_SEQS / NUM_WARPS;
    size_t smem = sizeof(st<bf16, CHUNK_SIZE, D_HEAD>) +                  // KV shared (reused!)
                  sizeof(st<bf16, MAX_N_SEQS, D_HEAD>) +                  // Q tile (MAX_N_SEQS)
                  sizeof(st<float, ROWS_PER_WARP, D_HEAD>) * NUM_WARPS +  // output per-warp
                  sizeof(sv<float, ROWS_PER_WARP>) * NUM_WARPS * 2 +      // max/sum vectors per-warp
                  16384;                                                  // Extra padding for safety
    smem = ((smem + 4095) / 4096) * 4096;

    cudaFuncSetAttribute(attn_chunk_first_tk<MAX_N_SEQS, CHUNK_SIZE, D_HEAD>,
                         cudaFuncAttributeMaxDynamicSharedMemorySize, smem);
    CUDA_CHECK();

    dim3 grid(n_heads, n_chunks);

    // Warmup
    for (int i = 0; i < warmup; i++) {
        attn_chunk_first_tk<MAX_N_SEQS, CHUNK_SIZE, D_HEAD><<<grid, BLOCK_THREADS, smem>>>(g);
    }
    CUDA_CHECK();

    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);

    cudaEventRecord(start);
    for (int i = 0; i < iters; i++) {
        attn_chunk_first_tk<MAX_N_SEQS, CHUNK_SIZE, D_HEAD><<<grid, BLOCK_THREADS, smem>>>(g);
    }
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float ms = 0.0f;
    cudaEventElapsedTime(&ms, start, stop);
    ms /= float(iters);

    cudaEventDestroy(start);
    cudaEventDestroy(stop);

    // Cleanup
    cudaFree(d_q);
    for (int c = 0; c < n_chunks; c++) { cudaFree(d_k[c]); cudaFree(d_v[c]); }
    cudaFree(d_attns); cudaFree(d_maxs); cudaFree(d_sums);
    cudaFree(d_keys); cudaFree(d_vals);
    cudaFree(d_offsets); cudaFree(d_begins); cudaFree(d_ends);

    return ms;
}

static void print_csv_header() {
    std::cout << "n_seqs,n_heads,n_chunks,blocks,latency_ms,tflops,compute_eff_pct,kv_gbps,kv_mem_eff_pct\n";
}

static void bench_and_print_row(int n_heads, int n_chunks, int warmup, int iters) {
    constexpr double H100_BF16_PEAK_TFLOPS = 989.0;
    constexpr double H100_HBM3_PEAK_GBPS = 3350.0;

    const int n_seqs = MAX_N_SEQS;
    const int blocks = n_heads * n_chunks;
    const float latency_ms = bench_kernel_ms(n_heads, n_chunks, warmup, iters);
    const double latency_s = double(latency_ms) * 1e-3;

    const double tflops = flops_total(n_seqs, n_heads, n_chunks) / latency_s / 1e12;
    const double compute_eff_pct = 100.0 * tflops / H100_BF16_PEAK_TFLOPS;
    const double kv_gbps = kv_bytes_total(n_heads, n_chunks) / latency_s / 1e9;
    const double kv_mem_eff_pct = 100.0 * kv_gbps / H100_HBM3_PEAK_GBPS;

    std::cout << n_seqs << "," << n_heads << "," << n_chunks << "," << blocks << ","
              << std::fixed << std::setprecision(6) << latency_ms << ","
              << std::setprecision(4) << tflops << ","
              << std::setprecision(4) << compute_eff_pct << ","
              << std::setprecision(2) << kv_gbps << ","
              << std::setprecision(4) << kv_mem_eff_pct
              << "\n";
}

// ============================================================================
// Test verification
// ============================================================================
struct TestConfig {
    int n_seqs, chunk_size, d_head, n_heads, n_chunks;
};

struct Stats {
    int fails = 0, nans = 0, total = 0;
    float max_abs = 0, sum_abs = 0;
    float max_rel = 0, sum_rel = 0;

    void add(float actual, float expected, float tol) {
        float abs_diff = std::abs(actual - expected);
        float rel_diff = (expected != 0) ? 100.0f * abs_diff / std::abs(expected) : 0;

        total++;
        max_abs = std::max(max_abs, abs_diff);
        sum_abs += abs_diff;
        max_rel = std::max(max_rel, rel_diff);
        sum_rel += rel_diff;

        if (std::isnan(actual) || std::isinf(actual)) nans++;
        else if (abs_diff > tol) fails++;
    }

    float avg_abs() const { return total > 0 ? sum_abs / total : 0; }
    float avg_rel() const { return total > 0 ? sum_rel / total : 0; }
    bool pass() const { return fails == 0 && nans == 0; }
};

bool verify(const TestConfig& cfg,
            const float* actual_attns, const float* actual_maxs, const float* actual_sums,
            const std::vector<std::vector<float>>& ref_attns,
            const std::vector<std::vector<float>>& ref_maxs,
            const std::vector<std::vector<float>>& ref_sums,
            const std::vector<int>& offsets,
            bool verbose) {

    Stats attn, maxs, sums;
    int first_errors = 0;

    for (int c = 0; c < cfg.n_chunks; c++) {
        int off = offsets[c];

        // Check attention outputs
        for (int h = 0; h < cfg.n_heads; h++) {
            for (int s = 0; s < cfg.n_seqs; s++) {
                for (int d = 0; d < cfg.d_head; d++) {
                    int local = h * cfg.n_seqs * cfg.d_head + s * cfg.d_head + d;
                    int global = off * cfg.n_heads * cfg.d_head + local;

                    float act = actual_attns[global];
                    float exp = ref_attns[c][local];
                    float tol = std::max(0.1f, std::abs(exp) * 0.03f);

                    int prev_fails = attn.fails + attn.nans;
                    attn.add(act, exp, tol);

                    if (verbose && (attn.fails + attn.nans) > prev_fails && first_errors < 5) {
                        std::cout << "  FAIL attn[c=" << c << " h=" << h << " s=" << s << " d=" << d
                                  << "] act=" << act << " exp=" << exp << " diff=" << std::abs(act-exp) << "\n";
                        first_errors++;
                    }
                }
            }
        }

        // Check max/sum
        for (int h = 0; h < cfg.n_heads; h++) {
            for (int s = 0; s < cfg.n_seqs; s++) {
                int local = h * cfg.n_seqs + s;
                int global = off * cfg.n_heads + local;

                float max_act = actual_maxs[global], max_exp = ref_maxs[c][local];
                float sum_act = actual_sums[global], sum_exp = ref_sums[c][local];

                maxs.add(max_act, max_exp, std::max(0.1f, std::abs(max_exp) * 0.03f));
                sums.add(sum_act, sum_exp, std::max(0.1f, std::abs(sum_exp) * 0.03f));
            }
        }
    }

    bool all_pass = attn.pass() && maxs.pass() && sums.pass();

    // Print results
    if (verbose || !all_pass) {
        std::cout << "\n=== Results ===\n";
        std::cout << "Attention: " << (attn.pass() ? "PASS" : "FAIL")
                  << " (fails=" << attn.fails << "/" << attn.total << " nan=" << attn.nans << ")\n"
                  << "  Abs: max=" << attn.max_abs << " avg=" << attn.avg_abs() << "\n"
                  << "  Rel: max=" << std::fixed << std::setprecision(2) << attn.max_rel
                  << "% avg=" << attn.avg_rel() << "%\n";
        std::cout << "Max values: " << (maxs.pass() ? "PASS" : "FAIL")
                  << " (fails=" << maxs.fails << "/" << maxs.total << " nan=" << maxs.nans << ")\n"
                  << "  Abs: max=" << maxs.max_abs << " avg=" << maxs.avg_abs() << "\n";
        std::cout << "Sum values: " << (sums.pass() ? "PASS" : "FAIL")
                  << " (fails=" << sums.fails << "/" << sums.total << " nan=" << sums.nans << ")\n"
                  << "  Abs: max=" << sums.max_abs << " avg=" << sums.avg_abs() << "\n";
    }

    std::cout << (all_pass ? "PASSED" : "FAILED") << "\n";

    // Summary line for test runner
    int total_fails = attn.fails + attn.nans;
    float fail_rate = 100.0f * total_fails / attn.total;
    std::cout << "SUMMARY: fails=" << total_fails << "/" << attn.total
              << " (" << std::fixed << std::setprecision(2) << fail_rate << "%)"
              << " max_diff=" << attn.max_abs << "\n";

    return all_pass;
}

// ============================================================================
// Main
// ============================================================================
int main(int argc, char** argv) {
    if (argc < 2) {
        std::cerr
          << "Usage:\n"
          << "  " << argv[0] << " <test.txt> [-v]\n"
          << "  " << argv[0] << " --bench --n-heads <H> --n-chunks <C> [--warmup N --iters N --device D]\n"
          << "  " << argv[0] << " --sweep [--warmup N --iters N --device D]\n";
        return 1;
    }

    set_device_from_flags(argc, argv);

    // Benchmark modes (kernel-only timing, prints CSV to stdout)
    if (strcmp(argv[1], "--bench") == 0) {
        const int n_heads = parse_int_flag(argc, argv, "--n-heads", -1);
        const int n_chunks = parse_int_flag(argc, argv, "--n-chunks", -1);
        const int warmup = parse_int_flag(argc, argv, "--warmup", 50);
        const int iters = parse_int_flag(argc, argv, "--iters", 200);
        if (n_heads <= 0 || n_chunks <= 0) {
            std::cerr << "Error: --bench requires --n-heads <H> and --n-chunks <C>\n";
            return 2;
        }
        print_csv_header();
        bench_and_print_row(n_heads, n_chunks, warmup, iters);
        return 0;
    }

    if (strcmp(argv[1], "--sweep") == 0) {
        const int warmup = parse_int_flag(argc, argv, "--warmup", 50);
        const int iters = parse_int_flag(argc, argv, "--iters", 200);
        print_csv_header();

        // Sweep points used by the existing result CSVs/plots in this repo.
        const struct { int h; int c; } sweep[] = {
          {4, 4}, {4, 8}, {4, 16}, {4, 32}, {4, 64},
          {16, 16}, {32, 8},
          {32, 16}, {8, 64},
          {64, 16}, {16, 64},
          {64, 64},
          {128, 64},
          {64, 128},
          {128, 128},
          {256, 128},
          {128, 256},
          {256, 256},
        };
        for (auto cfg : sweep) {
            bench_and_print_row(cfg.h, cfg.c, warmup, iters);
        }
        return 0;
    }

    bool verbose = has_flag(argc, argv, "-v");

    // Read test file
    std::ifstream f(argv[1]);
    if (!f) { std::cerr << "Cannot open " << argv[1] << "\n"; return 1; }

    TestConfig cfg;
    f >> cfg.n_seqs >> cfg.chunk_size >> cfg.d_head >> cfg.n_heads >> cfg.n_chunks;

    if (cfg.n_seqs > MAX_N_SEQS || cfg.chunk_size != CHUNK_SIZE || cfg.d_head != D_HEAD) {
        std::cerr << "Config mismatch: n_seqs=" << cfg.n_seqs << " (max " << MAX_N_SEQS << "), "
                  << "chunk_size=" << cfg.chunk_size << " (need " << CHUNK_SIZE << "), "
                  << "d_head=" << cfg.d_head << " (need " << D_HEAD << ")\n";
        return 1;
    }

    if (verbose) {
        std::cout << "Config: n_seqs=" << cfg.n_seqs << " n_heads=" << cfg.n_heads
                  << " n_chunks=" << cfg.n_chunks << "\n";
    }

    // Sizes
    const int q_size = cfg.n_seqs * cfg.n_heads * cfg.d_head;
    const int kv_size = cfg.n_heads * cfg.chunk_size * cfg.d_head;
    const int out_size = cfg.n_heads * cfg.n_seqs * cfg.d_head;
    const int ms_size = cfg.n_heads * cfg.n_seqs;

    // Read data
    std::vector<float> h_q(q_size);
    std::vector<std::vector<float>> h_k(cfg.n_chunks), h_v(cfg.n_chunks);
    std::vector<std::vector<float>> ref_out(cfg.n_chunks), ref_max(cfg.n_chunks), ref_sum(cfg.n_chunks);

    for (auto& x : h_q) f >> x;
    for (int c = 0; c < cfg.n_chunks; c++) { h_k[c].resize(kv_size); for (auto& x : h_k[c]) f >> x; }
    for (int c = 0; c < cfg.n_chunks; c++) { h_v[c].resize(kv_size); for (auto& x : h_v[c]) f >> x; }
    for (int c = 0; c < cfg.n_chunks; c++) { ref_out[c].resize(out_size); for (auto& x : ref_out[c]) f >> x; }
    for (int c = 0; c < cfg.n_chunks; c++) { ref_max[c].resize(ms_size); for (auto& x : ref_max[c]) f >> x; }
    for (int c = 0; c < cfg.n_chunks; c++) { ref_sum[c].resize(ms_size); for (auto& x : ref_sum[c]) f >> x; }

    // Allocate device memory
    bf16 *d_q;
    std::vector<bf16*> d_k(cfg.n_chunks), d_v(cfg.n_chunks);
    float *d_attns, *d_maxs, *d_sums;
    void **d_keys, **d_vals;
    int *d_offsets, *d_begins, *d_ends;

    cudaMalloc(&d_q, q_size * sizeof(bf16));
    for (int c = 0; c < cfg.n_chunks; c++) {
        cudaMalloc(&d_k[c], kv_size * sizeof(bf16));
        cudaMalloc(&d_v[c], kv_size * sizeof(bf16));
    }

    int total_out = cfg.n_chunks * out_size;
    int total_ms = cfg.n_chunks * ms_size;
    cudaMalloc(&d_attns, total_out * sizeof(float));
    cudaMalloc(&d_maxs, total_ms * sizeof(float));
    cudaMalloc(&d_sums, total_ms * sizeof(float));
    cudaMemset(d_attns, 0, total_out * sizeof(float));
    cudaMemset(d_maxs, 0, total_ms * sizeof(float));
    cudaMemset(d_sums, 0, total_ms * sizeof(float));

    // Convert to bf16 and copy
    std::vector<bf16> tmp(std::max(q_size, kv_size));
    // to_bf16(h_q.data(), tmp.data(), q_size); -- OLD: flat copy
    transpose_q_for_tma(h_q.data(), tmp.data(), cfg.n_seqs, cfg.n_heads, cfg.d_head); // NEW: transpose for TMA [1, H, N, D]
    cudaMemcpy(d_q, tmp.data(), q_size * sizeof(bf16), cudaMemcpyHostToDevice);

    for (int c = 0; c < cfg.n_chunks; c++) {
        to_bf16(h_k[c].data(), tmp.data(), kv_size);
        cudaMemcpy(d_k[c], tmp.data(), kv_size * sizeof(bf16), cudaMemcpyHostToDevice);
        to_bf16(h_v[c].data(), tmp.data(), kv_size);
        cudaMemcpy(d_v[c], tmp.data(), kv_size * sizeof(bf16), cudaMemcpyHostToDevice);
    }

    // Setup pointer arrays
    std::vector<void*> h_keys(cfg.n_chunks), h_vals(cfg.n_chunks);
    for (int c = 0; c < cfg.n_chunks; c++) { h_keys[c] = d_k[c]; h_vals[c] = d_v[c]; }
    cudaMalloc(&d_keys, cfg.n_chunks * sizeof(void*));
    cudaMalloc(&d_vals, cfg.n_chunks * sizeof(void*));
    cudaMemcpy(d_keys, h_keys.data(), cfg.n_chunks * sizeof(void*), cudaMemcpyHostToDevice);
    cudaMemcpy(d_vals, h_vals.data(), cfg.n_chunks * sizeof(void*), cudaMemcpyHostToDevice);

    // Setup metadata (each chunk covers all seqs)
    std::vector<int> h_begins(cfg.n_chunks, 0);
    std::vector<int> h_ends(cfg.n_chunks, cfg.n_seqs);
    std::vector<int> h_offsets(cfg.n_chunks);
    for (int c = 0; c < cfg.n_chunks; c++) h_offsets[c] = c * cfg.n_seqs;

    cudaMalloc(&d_begins, cfg.n_chunks * sizeof(int));
    cudaMalloc(&d_ends, cfg.n_chunks * sizeof(int));
    cudaMalloc(&d_offsets, cfg.n_chunks * sizeof(int));
    cudaMemcpy(d_begins, h_begins.data(), cfg.n_chunks * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_ends, h_ends.data(), cfg.n_chunks * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_offsets, h_offsets.data(), cfg.n_chunks * sizeof(int), cudaMemcpyHostToDevice);
    CUDA_CHECK();

    // Setup kernel globals
    using Globals = attn_chunk_first_globals<MAX_N_SEQS, D_HEAD>;
    Globals g {
        .attns = gl<float, -1, -1, -1, -1>(d_attns, 1, 1, total_out / D_HEAD, D_HEAD),
        .maxs = gl<float, -1, -1, -1, -1>(d_maxs, 1, 1, 1, total_ms),
        .sums = gl<float, -1, -1, -1, -1>(d_sums, 1, 1, 1, total_ms),
        .offsets = gl<int, -1, -1, -1, -1>(d_offsets, 1, 1, 1, cfg.n_chunks),
        .begins = gl<int, -1, -1, -1, -1>(d_begins, 1, 1, 1, cfg.n_chunks),
        .ends = gl<int, -1, -1, -1, -1>(d_ends, 1, 1, 1, cfg.n_chunks),
        .Q = make_gl<Globals::Q_gl>((uint64_t)d_q, 1, cfg.n_heads, cfg.n_seqs, D_HEAD),
        .keys = d_keys,
        .values = d_vals,
        .scale = 1.0f / sqrtf(float(cfg.d_head)),
        .n_heads = cfg.n_heads
    };

    // Compute shared memory size
    // NOTE: K and V share the same buffer (reused sequentially)
    constexpr int NUM_WARPS = 4;
    constexpr int ROWS_PER_WARP = MAX_N_SEQS / NUM_WARPS;
    size_t smem = sizeof(st<bf16, CHUNK_SIZE, D_HEAD>) +                  // KV shared (reused!)
                  sizeof(st<bf16, MAX_N_SEQS, D_HEAD>) +                  // Q tile (MAX_N_SEQS)
                  sizeof(st<float, ROWS_PER_WARP, D_HEAD>) * NUM_WARPS +  // output per-warp
                  sizeof(sv<float, ROWS_PER_WARP>) * NUM_WARPS * 2 +      // max/sum vectors per-warp
                  16384;                                                  // Extra padding for safety
    smem = ((smem + 4095) / 4096) * 4096;

    cudaFuncSetAttribute(attn_chunk_first_tk<MAX_N_SEQS, CHUNK_SIZE, D_HEAD>,
                         cudaFuncAttributeMaxDynamicSharedMemorySize, smem);
    CUDA_CHECK();

    // Launch
    dim3 grid(cfg.n_heads, cfg.n_chunks);
    attn_chunk_first_tk<MAX_N_SEQS, CHUNK_SIZE, D_HEAD><<<grid, BLOCK_THREADS, smem>>>(g);
    CUDA_CHECK();

    // Copy results back
    std::vector<float> h_attns(total_out), h_maxs(total_ms), h_sums(total_ms);
    cudaMemcpy(h_attns.data(), d_attns, total_out * sizeof(float), cudaMemcpyDeviceToHost);
    cudaMemcpy(h_maxs.data(), d_maxs, total_ms * sizeof(float), cudaMemcpyDeviceToHost);
    cudaMemcpy(h_sums.data(), d_sums, total_ms * sizeof(float), cudaMemcpyDeviceToHost);

    // Verify
    bool pass = verify(cfg, h_attns.data(), h_maxs.data(), h_sums.data(),
                       ref_out, ref_max, ref_sum, h_offsets, verbose);

    // Cleanup
    cudaFree(d_q);
    for (int c = 0; c < cfg.n_chunks; c++) { cudaFree(d_k[c]); cudaFree(d_v[c]); }
    cudaFree(d_attns); cudaFree(d_maxs); cudaFree(d_sums);
    cudaFree(d_keys); cudaFree(d_vals);
    cudaFree(d_offsets); cudaFree(d_begins); cudaFree(d_ends);

    return pass ? 0 : 1;
}
